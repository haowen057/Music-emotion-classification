import os
import torch
from torch.utils.data import Dataset
import pandas as pd
import torchaudio
import torchaudio.transforms as T
import torch.nn as nn
from AudioAugmentation import CombinedAugmentation 
import logging
import numpy as np

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class EmotionSoundDataset(Dataset):
    def __init__(self,
                 annotations_file,    # æ–‡ä»¶ä¿¡æ¯è·¯å¾„
                 audio_dir,           # éŸ³é¢‘æ ¹ç›®å½•
                 transformation=None, # æ•°æ®å˜æ¢
                 target_sample_rate=22050,
                 num_samples=220500,
                 device='cpu',
                 use_log_mel=True,
                 top_n_classes=10,
                 augment=False,
                 # ğŸ¯ æ–°å¢ï¼šç»†ç²’åº¦å¢å¼ºæ§åˆ¶
                 audio_augment=True,  
                 spec_augment=True,
                 # ğŸ¯ æ–°å¢ï¼šç¡®å®šæ€§å¢å¼ºæ§åˆ¶
                 deterministic_augment=True,
                 augment_seed=42
                 ):
        
        self.device = device
        self.transformation = transformation
        self.target_sample_rate = target_sample_rate
        self.num_samples = num_samples
        self.use_log_mel = use_log_mel
        self.audio_dir = audio_dir
        self.augment = augment
        self.audio_augment = audio_augment
        self.spec_augment = spec_augment
        self.deterministic_augment = deterministic_augment
        self.augment_seed = augment_seed

        # ğŸ¯ ä¼˜åŒ–ï¼šæ›´æ¸…æ™°çš„å¢å¼ºåˆå§‹åŒ–
        self.combined_augmentation = None

        if self.augment:
            self.combined_augmentation = CombinedAugmentation(
                sample_rate=target_sample_rate, 
                device=device,
                deterministic=deterministic_augment,  # ğŸ¯ ä½¿ç”¨ç¡®å®šæ€§æ¨¡å¼
                seed=augment_seed,
                audio_augment=self.audio_augment,    # ğŸ¯ æ–°å¢ï¼šä¼ é€’éŸ³é¢‘å¢å¼ºæ§åˆ¶
                spec_augment=self.spec_augment       # ğŸ¯ æ–°å¢ï¼šä¼ é€’é¢‘è°±å¢å¼ºæ§åˆ¶
            )
            mode = "ç¡®å®šæ€§" if deterministic_augment else "éšæœº"
            logger.info(f"âœ… åˆå§‹åŒ–å¢å¼º: éŸ³é¢‘å¢å¼º={audio_augment}, é¢‘è°±å¢å¼º={spec_augment}, æ¨¡å¼={mode}, ç§å­={augment_seed}")

        # å‰ 10 ç±»æ ‡ç­¾
        self.top_labels = [
            'melodic', 'energetic', 'dark', 'film', 'relaxing',
            'dream', 'ambiental', 'love', 'soundscape', 'emotional'
        ][:top_n_classes]

        # è¯»å–æ–‡ä»¶ä¿¡æ¯
        self.annotations = pd.read_csv(annotations_file, sep='\t')

        # åªä¿ç•™å‰ n ç±»çš„æ ·æœ¬
        self.filtered_annotations = self.annotations[
            self.annotations['TAG'].apply(
                lambda x: any(tag.replace('mood/theme---','').strip() in self.top_labels for tag in x.split(','))
            )
        ].reset_index(drop=True)

        # mel å˜æ¢
        self.mel_transform = T.MelSpectrogram(
            sample_rate=target_sample_rate,
            n_fft=1024,
            hop_length=512,
            n_mels=64
        ).to(self.device)

        print(f"ğŸ¯ æ€»æ ·æœ¬æ•°: {len(self.filtered_annotations)}")
        for label in self.top_labels:
            count = len(self.filtered_annotations[self.filtered_annotations['TAG'].str.contains(label)])
            print(f"âœ… mood/theme---{label}: æ‰¾åˆ° {count} ä¸ª wav æ–‡ä»¶")

    def __len__(self):
        return len(self.filtered_annotations)
    
    def __getitem__(self, index):
        audio_sample_path = self._get_audio_sample_path(index)
        label = self._get_audio_sample_label(index)
        if label == -1:
            raise ValueError(f"Sample {index} not in top {len(self.top_labels)} labels")

        signal, sr = torchaudio.load(audio_sample_path)
        signal = signal.to(self.device)

        # ğŸ¯ ä¼˜åŒ–ï¼šç»Ÿä¸€çš„é¢„å¤„ç†æµç¨‹
        signal = self._preprocess_audio(signal, sr)

        # ğŸ¯ ä¼˜åŒ–ï¼šæ›´æ¸…æ™°çš„å¢å¼ºé€»è¾‘
        if self.augment and self.combined_augmentation:
            # åº”ç”¨éŸ³é¢‘å¢å¼º
            if self.audio_augment:
                try:
                    # ğŸ¯ ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨æ–°çš„ç¡®å®šæ€§å¢å¼ºæ¥å£
                    signal = self.combined_augmentation(signal, index=index)
                except Exception as e:
                    logger.warning(f"éŸ³é¢‘å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ä¿¡å·: {e}")
            
            # è½¬æ¢ä¸ºæ¢…å°”é¢‘è°±å›¾
            mel_spectrogram = self.mel_transform(signal)
            features = torch.log(mel_spectrogram + 1e-9)
            
            # åº”ç”¨é¢‘è°±å¢å¼º
            # if self.spec_augment:
            #     try:
            #         # ğŸ¯ é¢‘è°±å¢å¼ºä¿æŒéšæœºæ€§ï¼ˆåœ¨è®­ç»ƒæ—¶åº”ç”¨ï¼‰
            #         features = self.combined_augmentation(features)
            #     except Exception as e:
            #         logger.warning(f"é¢‘è°±å¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é¢‘è°±: {e}")
        else:
            # æ— å¢å¼ºè·¯å¾„
            mel_spectrogram = self.mel_transform(signal)
            features = torch.log(mel_spectrogram + 1e-9)

        # åŸæœ‰çš„å˜æ¢
        if self.transformation:
            features = self.transformation(features)

        return features, label

    # ğŸ¯ æ–°å¢ï¼šç»Ÿä¸€çš„é¢„å¤„ç†æ–¹æ³•
    def _preprocess_audio(self, signal, sr):
        """ç»Ÿä¸€çš„éŸ³é¢‘é¢„å¤„ç†æµç¨‹"""
        signal = self._resample_if_necessary(signal, sr)
        signal = self._mix_down_if_necessary(signal)
        signal = self._cut_if_necessary(signal)
        signal = self._right_pad_if_necessary(signal)
        return signal

    # -------------------- è¾…åŠ©å‡½æ•° --------------------
    def _get_audio_sample_label(self, index):
        tags_str = self.filtered_annotations.iloc[index]['TAG']
        tags = tags_str.split(',')
        for tag in tags:
            clean_tag = tag.replace('mood/theme---', '').strip()
            if clean_tag in self.top_labels:
                return self.top_labels.index(clean_tag)
        return -1

    def _get_audio_sample_path(self, index):
        tags_str = self.filtered_annotations.iloc[index]['TAG']
        tags = tags_str.split(',')
        chosen_tag = None
        for tag in tags:
            clean_tag = tag.replace('mood/theme---', '').strip()
            if clean_tag in self.top_labels:
                chosen_tag = clean_tag
                break
        if chosen_tag is None:
            raise ValueError(f"Sample {index} has no valid top label")
        filename = self.filtered_annotations.iloc[index]['PATH']
        full_path = os.path.join(self.audio_dir, filename)
        if not os.path.exists(full_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {full_path}")
        return full_path
    
    def _cut_if_necessary(self, signal):
        return signal[:, :self.num_samples] if signal.shape[1] > self.num_samples else signal

    def _right_pad_if_necessary(self, signal):
        if signal.shape[1] < self.num_samples:
            pad = self.num_samples - signal.shape[1]
            signal = nn.functional.pad(signal, (0, pad))
        return signal

    def _resample_if_necessary(self, signal, sr):
        if sr != self.target_sample_rate:
            resampler = T.Resample(sr, self.target_sample_rate).to(self.device)
            signal = resampler(signal)
        return signal

    def _mix_down_if_necessary(self, signal):
        if signal.shape[0] > 1:
            signal = torch.mean(signal, dim=0, keepdim=True)
        return signal
    
    # ğŸ¯ ä¼˜åŒ–ï¼šå¢å¼ºçš„è°ƒè¯•æ–¹æ³•
    def get_audio_and_mel(self, index, apply_augmentation=True):
        """åˆ†åˆ«è·å–éŸ³é¢‘å’Œæ¢…å°”é¢‘è°±å›¾ï¼Œç”¨äºè°ƒè¯•å’Œå¯è§†åŒ–"""
        audio_sample_path = self._get_audio_sample_path(index)
        label = self._get_audio_sample_label(index)
        if label == -1:
            raise ValueError(f"Sample {index} not in top {len(self.top_labels)} labels")
    
        signal, sr = torchaudio.load(audio_sample_path)
        signal = signal.to(self.device)
    
        # é¢„å¤„ç†
        original_signal = self._preprocess_audio(signal, sr)
        
        # åº”ç”¨éŸ³é¢‘å¢å¼º
        if apply_augmentation and self.augment and self.audio_augment:
            try:
                # ğŸ¯ ä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„ç¡®å®šæ€§å¢å¼ºæ¥å£
                augmented_signal = self.combined_augmentation(original_signal.clone(), index=index)
            except Exception as e:
                logger.warning(f"è°ƒè¯•æ¨¡å¼éŸ³é¢‘å¢å¼ºå¤±è´¥: {e}")
                augmented_signal = original_signal
        else:
            augmented_signal = original_signal
    
        # è½¬æ¢ä¸ºæ¢…å°”é¢‘è°±å›¾
        mel_spectrogram = self.mel_transform(augmented_signal)
        features = torch.log(mel_spectrogram + 1e-9)
    
        # åº”ç”¨é¢‘è°±å›¾å¢å¼º
        if apply_augmentation and self.augment and self.spec_augment:
            try:
                features = self.combined_augmentation(features)
            except Exception as e:
                logger.warning(f"è°ƒè¯•æ¨¡å¼é¢‘è°±å¢å¼ºå¤±è´¥: {e}")
    
        return original_signal, augmented_signal, features, label
    
    # ğŸ¯ æ–°å¢ï¼šåˆ‡æ¢å¢å¼ºæ¨¡å¼çš„æ–¹æ³•
    def set_augmentation_mode(self, deterministic=True, seed=42):
        """åˆ‡æ¢å¢å¼ºæ¨¡å¼ï¼ˆç¡®å®šæ€§/éšæœºï¼‰"""
        if self.combined_augmentation:
            self.combined_augmentation.toggle_deterministic(deterministic, seed)
            self.deterministic_augment = deterministic
            self.augment_seed = seed
            mode = "ç¡®å®šæ€§" if deterministic else "éšæœº"
            logger.info(f"ğŸ¯ åˆ‡æ¢å¢å¼ºæ¨¡å¼: {mode}, ç§å­: {seed}")
    

    
    # ğŸ¯ æ–°å¢ï¼šè·å–æ•°æ®é›†ä¿¡æ¯
    def get_dataset_info(self):
        """è¿”å›æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        info = {
            'total_samples': len(self.filtered_annotations),
            'sample_rate': self.target_sample_rate,
            'num_samples': self.num_samples,
            'augment_enabled': self.augment,
            'audio_augment': self.audio_augment,
            'spec_augment': self.spec_augment,
            'deterministic_augment': self.deterministic_augment,
            'augment_seed': self.augment_seed,
            'labels_distribution': {}
        }
        
        for label in self.top_labels:
            count = len(self.filtered_annotations[self.filtered_annotations['TAG'].str.contains(label)])
            info['labels_distribution'][label] = count
            
        return info