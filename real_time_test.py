import torch
import torchaudio
import numpy as np
import pyaudio
import threading
import time
from collections import deque
from cnn import CNNNetwork10
from Train_Final import Config

class TenSecondEmotionClassifier:
    def __init__(self, model_path, sample_rate=22050):
        """
        10ç§’éŸ³ä¹æƒ…ç»ªåˆ†ç±»å™¨
        
        å‚æ•°:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            sample_rate: é‡‡æ ·ç‡ (ä¸è®­ç»ƒæ—¶ä¸€è‡´)
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.sample_rate = sample_rate
        self.chunk_size = sample_rate * 10  # 10ç§’éŸ³é¢‘
        
        # åŠ è½½æ¨¡å‹
        self.model = self.load_model(model_path)
        self.model.eval()
        
        # æƒ…ç»ªæ ‡ç­¾ (ä¸è®­ç»ƒæ—¶ä¸€è‡´)
        self.emotion_labels = Config.TOP10_TAGS
        
        # éŸ³é¢‘ç¼“å†²åŒº - å­˜å‚¨10ç§’æ•°æ®
        self.audio_buffer = deque(maxlen=self.chunk_size)
        self.is_listening = False
        self.audio_interface = None
        self.stream = None
        
        # åˆ†æçŠ¶æ€
        self.is_analyzing = False
        self.last_analysis_time = 0
        self.analysis_interval = 10  # æ¯10ç§’åˆ†æä¸€æ¬¡
        
        print(f"ğŸµ 10ç§’éŸ³ä¹æƒ…ç»ªåˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š è®¾å¤‡: {self.device}")
        print(f"â±ï¸  åˆ†ææ—¶é•¿: 10ç§’")
        print(f"ğŸ¯ æ”¯æŒçš„æƒ…ç»ªç±»åˆ«: {self.emotion_labels}")
    
    def load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model = CNNNetwork10(num_classes=len(Config.TOP10_TAGS)).to(self.device)
        
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ ¼å¼
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
            
            model.load_state_dict(state_dict)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
        return model
    
    def preprocess_audio(self, audio_chunk):
        """
        éŸ³é¢‘é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        """
        # è½¬æ¢ä¸ºå¼ é‡
        if isinstance(audio_chunk, np.ndarray):
            audio_tensor = torch.from_numpy(audio_chunk).float()
        else:
            audio_tensor = audio_chunk.clone().detach().float()
        
        # ç¡®ä¿å½¢çŠ¶ä¸º [1, samples]
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)
        
        # è£å‰ªæˆ–å¡«å……åˆ°10ç§’é•¿åº¦
        if audio_tensor.shape[1] > self.chunk_size:
            audio_tensor = audio_tensor[:, :self.chunk_size]
        elif audio_tensor.shape[1] < self.chunk_size:
            pad_size = self.chunk_size - audio_tensor.shape[1]
            audio_tensor = torch.nn.functional.pad(audio_tensor, (0, pad_size))
        
        return audio_tensor.to(self.device)
    
    def extract_logmel_features(self, audio_tensor):
        """
        æå–Log-Melç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        """
        mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=self.sample_rate,
            n_fft=1024,
            hop_length=512,
            n_mels=64
        ).to(self.device)
        
        mel_spectrogram = mel_transform(audio_tensor)
        logmel_features = torch.log(mel_spectrogram + 1e-9)
        
        return logmel_features.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
    
    def classify_10s_emotion(self, audio_chunk):
        """
        åˆ†ç±»10ç§’éŸ³é¢‘ç‰‡æ®µçš„æƒ…ç»ª
        """
        with torch.no_grad():
            try:
                # æ£€æŸ¥éŸ³é¢‘é•¿åº¦
                if len(audio_chunk) < self.chunk_size:
                    print(f"âš ï¸ éŸ³é¢‘é•¿åº¦ä¸è¶³: {len(audio_chunk)}/{self.chunk_size} é‡‡æ ·ç‚¹")
                    return "Insufficient Audio", 0.0
                
                # é¢„å¤„ç†
                processed_audio = self.preprocess_audio(audio_chunk)
                
                # æå–ç‰¹å¾
                features = self.extract_logmel_features(processed_audio)
                
                # é¢„æµ‹
                outputs = self.model(features)
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                emotion_idx = predicted.item()
                confidence_score = confidence.item()
                
                return self.emotion_labels[emotion_idx], confidence_score
                
            except Exception as e:
                print(f"âŒ åˆ†ç±»å¤±è´¥: {e}")
                return "Classification Error", 0.0
    
    def audio_callback(self, in_data, frame_count, time_info, status):
        """
        éŸ³é¢‘æµå›è°ƒå‡½æ•°
        """
        # å°†éŸ³é¢‘æ•°æ®æ·»åŠ åˆ°ç¼“å†²åŒº
        audio_data = np.frombuffer(in_data, dtype=np.float32)
        self.audio_buffer.extend(audio_data)
        
        return (in_data, pyaudio.paContinue)
    
    def start_analysis_loop(self):
        """
        å¼€å§‹åˆ†æå¾ªç¯ï¼ˆåœ¨å•ç‹¬çº¿ç¨‹ä¸­è¿è¡Œï¼‰
        """
        def analysis_loop():
            buffer_fill_start = None
            
            while self.is_listening:
                current_buffer_size = len(self.audio_buffer)
                
                # æ˜¾ç¤ºç¼“å†²åŒºå¡«å……è¿›åº¦
                if current_buffer_size < self.chunk_size:
                    if buffer_fill_start is None:
                        buffer_fill_start = time.time()
                        print(f"ğŸ”„ æ­£åœ¨å¡«å……éŸ³é¢‘ç¼“å†²åŒº... ({current_buffer_size}/{self.chunk_size})")
                    else:
                        progress = (current_buffer_size / self.chunk_size) * 100
                        if int(progress) % 10 == 0:  # æ¯10%æ˜¾ç¤ºä¸€æ¬¡
                            print(f"ğŸ”„ ç¼“å†²åŒºå¡«å……: {progress:.0f}% ({current_buffer_size}/{self.chunk_size})")
                
                # å½“ç¼“å†²åŒºæœ‰10ç§’æ•°æ®æ—¶è¿›è¡Œåˆ†æ
                if current_buffer_size >= self.chunk_size:
                    if buffer_fill_start:
                        fill_time = time.time() - buffer_fill_start
                        print(f"âœ… ç¼“å†²åŒºå¡«å……å®Œæˆ! è€—æ—¶: {fill_time:.1f}ç§’")
                        buffer_fill_start = None
                    
                    # è·å–10ç§’éŸ³é¢‘æ•°æ®
                    chunk = np.array(list(self.audio_buffer))[-self.chunk_size:]
                    
                    print("ğŸ” æ­£åœ¨åˆ†æ10ç§’éŸ³é¢‘...")
                    start_time = time.time()
                    
                    # åˆ†ç±»æƒ…ç»ª
                    emotion, confidence = self.classify_10s_emotion(chunk)
                    
                    analysis_time = time.time() - start_time
                    
                    # æ˜¾ç¤ºç»“æœ
                    print("\n" + "="*60)
                    print(f"ğŸµ 10ç§’éŸ³ä¹æƒ…ç»ªåˆ†æç»“æœ:")
                    print(f"ğŸ“Š æ£€æµ‹æƒ…ç»ª: {emotion}")
                    print(f"ğŸ¯ ç½®ä¿¡åº¦: {confidence:.3f}")
                    print(f"â±ï¸  åˆ†æè€—æ—¶: {analysis_time:.2f}ç§’")
                    print(f"ğŸ•’ æ—¶é—´: {time.strftime('%H:%M:%S')}")
                    print("="*60 + "\n")
                    
                    # æ¸…ç©ºç¼“å†²åŒºï¼Œé‡æ–°å¼€å§‹æ”¶é›†
                    self.audio_buffer.clear()
                    buffer_fill_start = time.time()
                
                time.sleep(0.1)  # å‡å°‘CPUå ç”¨
        
        # å¯åŠ¨åˆ†æçº¿ç¨‹
        self.analysis_thread = threading.Thread(target=analysis_loop)
        self.analysis_thread.daemon = True
        self.analysis_thread.start()
    
    def start_listening(self):
        """
        å¼€å§‹ä»æ‰¬å£°å™¨å®æ—¶ç›‘å¬
        """
        try:
            self.audio_interface = pyaudio.PyAudio()
            
            # æ‰“å¼€éŸ³é¢‘æµï¼ˆä»æ‰¬å£°å™¨/ç«‹ä½“å£°æ··éŸ³è¾“å…¥ï¼‰
            self.stream = self.audio_interface.open(
                format=pyaudio.paFloat32,
                channels=1,  # å•å£°é“
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=1024,
                stream_callback=self.audio_callback,
                input_device_index=None  # ä½¿ç”¨é»˜è®¤è¾“å…¥è®¾å¤‡
            )
            
            self.is_listening = True
            self.stream.start_stream()
            
            print("ğŸ”Š å¼€å§‹10ç§’éŸ³ä¹æƒ…ç»ªæ£€æµ‹...")
            print("ğŸ’¡ è¯·æ’­æ”¾éŸ³ä¹ï¼Œç³»ç»Ÿå°†æ¯10ç§’åˆ†æä¸€æ¬¡")
            print("â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æ£€æµ‹")
            print("-" * 50)
            
            # å¯åŠ¨åˆ†æå¾ªç¯
            self.start_analysis_loop()
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘è®¾å¤‡åˆå§‹åŒ–å¤±è´¥: {e}")
            self.print_troubleshooting_guide()
    
    def print_troubleshooting_guide(self):
        """æ‰“å°æ•…éšœæ’é™¤æŒ‡å—"""
        print("\nğŸ”§ æ•…éšœæ’é™¤æŒ‡å—:")
        print("1. Windows: åœ¨å£°éŸ³è®¾ç½®ä¸­å¯ç”¨'ç«‹ä½“å£°æ··éŸ³'")
        print("2. Mac: å®‰è£…Soundfloweræˆ–BlackHoleè¿›è¡ŒéŸ³é¢‘è·¯ç”±")
        print("3. æ£€æŸ¥éº¦å…‹é£/å½•éŸ³æƒé™")
        print("4. ç¡®ä¿éŸ³é¢‘é©±åŠ¨æ­£å¸¸")
        print("5. å°è¯•ä¸åŒçš„è¾“å…¥è®¾å¤‡ç´¢å¼•")
    
    def stop_listening(self):
        """åœæ­¢ç›‘å¬"""
        self.is_listening = False
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        if self.audio_interface:
            self.audio_interface.terminate()
        print("ğŸ›‘ éŸ³ä¹æƒ…ç»ªæ£€æµ‹å·²åœæ­¢")

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================
def main():
    # æ¨¡å‹è·¯å¾„
    MODEL_PATH = "best_emotion_classifier_amp_enhanced_11.pth"
    
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = TenSecondEmotionClassifier(model_path=MODEL_PATH)
    
    try:
        # å¼€å§‹ç›‘å¬å’Œåˆ†æ
        classifier.start_listening()
        
        # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œ
        while classifier.is_listening:
            time.sleep(1)
                
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ£€æµ‹")
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")
    finally:
        classifier.stop_listening()

def test_with_pre_recorded_audio(audio_file_path):
    """
    ä½¿ç”¨é¢„å½•éŸ³é¢‘æ–‡ä»¶æµ‹è¯•åˆ†ç±»å™¨
    """
    classifier = TenSecondEmotionClassifier("best_emotion_classifier_amp_enhanced_11.pth")
    
    try:
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶
        audio, sr = torchaudio.load(audio_file_path)
        if sr != classifier.sample_rate:
            resampler = torchaudio.transforms.Resample(sr, classifier.sample_rate)
            audio = resampler(audio)
        
        # è½¬æ¢ä¸ºå•å£°é“
        if audio.shape[0] > 1:
            audio = torch.mean(audio, dim=0, keepdim=True)
        
        # ç¡®ä¿éŸ³é¢‘é•¿åº¦è¶³å¤Ÿ
        if audio.shape[1] < classifier.chunk_size:
            print(f"âš ï¸ éŸ³é¢‘æ–‡ä»¶è¿‡çŸ­: {audio.shape[1]/classifier.sample_rate:.1f}ç§’")
            return
        
        # åˆ†æå‰10ç§’
        chunk = audio[:, :classifier.chunk_size]
        
        print("ğŸ” åˆ†æéŸ³é¢‘æ–‡ä»¶...")
        emotion, confidence = classifier.classify_10s_emotion(chunk.numpy())
        
        print("\n" + "="*50)
        print(f"ğŸµ éŸ³é¢‘æ–‡ä»¶åˆ†æç»“æœ:")
        print(f"ğŸ“Š æ£€æµ‹æƒ…ç»ª: {emotion}")
        print(f"ğŸ¯ ç½®ä¿¡åº¦: {confidence:.3f}")
        print("="*50)
            
    except Exception as e:
        print(f"âŒ æ–‡ä»¶æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    # è¿è¡Œå®æ—¶10ç§’æ£€æµ‹
    main()
    
    # å–æ¶ˆæ³¨é‡Šä»¥æµ‹è¯•éŸ³é¢‘æ–‡ä»¶
    # test_with_pre_recorded_audio("your_audio_file.wav")